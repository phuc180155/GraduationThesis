{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Pretrained backbone:  True\n",
      "use vanilla attention\n",
      "use vanilla attention\n",
      "use vanilla attention\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 1, 3, 3], expected input[32, 3, 129, 129] to have 1 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/test_4.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 258>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/test_4.ipynb#ch0000000vscode-remote?line=258'>259</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(\u001b[39m32\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m128\u001b[39m, \u001b[39m128\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/test_4.ipynb#ch0000000vscode-remote?line=259'>260</a>\u001b[0m model_ \u001b[39m=\u001b[39m PairwiseCNNCMultiViT(image_size\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m, num_classes\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, \\\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/test_4.ipynb#ch0000000vscode-remote?line=260'>261</a>\u001b[0m             dim\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m, depth\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, heads\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, mlp_dim\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m, dim_head\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, dropout\u001b[39m=\u001b[39m\u001b[39m0.15\u001b[39m,\\\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/test_4.ipynb#ch0000000vscode-remote?line=261'>262</a>\u001b[0m             backbone\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mefficient_net\u001b[39m\u001b[39m'\u001b[39m, pretrained\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,unfreeze_blocks\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/test_4.ipynb#ch0000000vscode-remote?line=264'>265</a>\u001b[0m             dropout_in_mlp\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m, residual\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, transformer_shareweight\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, useKNN\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \\\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/test_4.ipynb#ch0000000vscode-remote?line=265'>266</a>\u001b[0m             embedding_return\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmlp_hidden\u001b[39m\u001b[39m'\u001b[39m, freq_stream\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrgb\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/test_4.ipynb#ch0000000vscode-remote?line=267'>268</a>\u001b[0m e0, out0, e1, out1 \u001b[39m=\u001b[39m model_(x, x)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/test_4.ipynb#ch0000000vscode-remote?line=268'>269</a>\u001b[0m \u001b[39mprint\u001b[39m(e0\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/test_4.ipynb#ch0000000vscode-remote?line=269'>270</a>\u001b[0m \u001b[39mprint\u001b[39m(out0\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m/mnt/disk1/anaconda3/envs/phucnp/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "\u001b[1;32m/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/test_4.ipynb Cell 1\u001b[0m in \u001b[0;36mPairwiseCNNCMultiViT.forward\u001b[0;34m(self, imgs0, imgs1)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/test_4.ipynb#ch0000000vscode-remote?line=251'>252</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, imgs0, imgs1):\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/test_4.ipynb#ch0000000vscode-remote?line=252'>253</a>\u001b[0m     embedding_0, out_0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_once(imgs0)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/test_4.ipynb#ch0000000vscode-remote?line=253'>254</a>\u001b[0m     embedding_1, out_1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_once(imgs1)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/test_4.ipynb#ch0000000vscode-remote?line=254'>255</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m embedding_0, out_0, embedding_1, out_1\n",
      "\u001b[1;32m/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/test_4.ipynb Cell 1\u001b[0m in \u001b[0;36mPairwiseCNNCMultiViT.forward_once\u001b[0;34m(self, imgs)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/test_4.ipynb#ch0000000vscode-remote?line=229'>230</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_once\u001b[39m(\u001b[39mself\u001b[39m, imgs):\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/test_4.ipynb#ch0000000vscode-remote?line=230'>231</a>\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract_feature(imgs)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/test_4.ipynb#ch0000000vscode-remote?line=231'>232</a>\u001b[0m     \u001b[39m# print(\"Features shape: \", features.shape)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/test_4.ipynb#ch0000000vscode-remote?line=232'>233</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/test_4.ipynb#ch0000000vscode-remote?line=233'>234</a>\u001b[0m     \u001b[39m##### Forward to ViT\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/test_4.ipynb#ch0000000vscode-remote?line=234'>235</a>\u001b[0m     e \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/test_4.ipynb Cell 1\u001b[0m in \u001b[0;36mPairwiseCNNCMultiViT.extract_feature\u001b[0;34m(self, imgs)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/test_4.ipynb#ch0000000vscode-remote?line=222'>223</a>\u001b[0m         features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextractor\u001b[39m.\u001b[39mextract_features(imgs)                 \u001b[39m# shape (batchsize, 1280, 8, 8)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/test_4.ipynb#ch0000000vscode-remote?line=223'>224</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/test_4.ipynb#ch0000000vscode-remote?line=224'>225</a>\u001b[0m         features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextractor\u001b[39m.\u001b[39;49mextract_features_at_block(imgs, \u001b[39mint\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures_at_block))\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/test_4.ipynb#ch0000000vscode-remote?line=225'>226</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/test_4.ipynb#ch0000000vscode-remote?line=226'>227</a>\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextractor(imgs)\n",
      "File \u001b[0;32m~/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/model/backbone/efficient_net/model.py:318\u001b[0m, in \u001b[0;36mEfficientNet.extract_features_at_block\u001b[0;34m(self, inputs, selected_block)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39m\"\"\"use convolution layer to extract feature .\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[39m    inputs (tensor): Input tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39m    layer in the efficientnet model.\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39m# Stem\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_swish(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bn0(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_stem(inputs)))\n\u001b[1;32m    320\u001b[0m \u001b[39m# Blocks\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[39mfor\u001b[39;00m idx, block \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blocks):\n",
      "File \u001b[0;32m/mnt/disk1/anaconda3/envs/phucnp/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/model/backbone/efficient_net/utils.py:144\u001b[0m, in \u001b[0;36mConv2dStaticSamePadding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    143\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatic_padding(x)\n\u001b[0;32m--> 144\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mconv2d(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n\u001b[1;32m    145\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 1, 3, 3], expected input[32, 3, 129, 129] to have 1 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch import einsum\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from einops import rearrange\n",
    "\n",
    "import sys\n",
    "from model.backbone.efficient_net.model import EfficientNet\n",
    "\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import re, math\n",
    "from model.vision_transformer.vit.vit import ViT, Transformer\n",
    "from model.vision_transformer.vit.kvit import kNNTransformer\n",
    "from model.vision_transformer.cnn_vit.efficient_vit import EfficientViT\n",
    "from pytorchcv.model_provider import get_model\n",
    "\n",
    "\n",
    "\n",
    "class CMultiscaleViT(nn.Module):\n",
    "    def __init__(self, in_channels=112, in_size=8, patch_reso='1-2-4-8', gamma_reso='0.8_0.4_0.2_0.1', residual=True,\\\n",
    "                useKNN=True, depth=6, heads=8, dim=1024, mlp_dim=2048, dim_head=64, dropout=0.15, share_weight=True):\n",
    "        super(CMultiscaleViT, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.heads = heads\n",
    "        self.dim_head = dim_head\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.dropout_value = dropout\n",
    "\n",
    "        self.residual = residual\n",
    "        self.patch_reso = patch_reso\n",
    "        self.gamma_reso = gamma_reso\n",
    "\n",
    "        self.patch_size = list(map(int, patch_reso.split('-')))\n",
    "        self.gamma_reso = list(map(float, gamma_reso.split('_')))\n",
    "        self.gamma = []\n",
    "        self.g0 = nn.Parameter(torch.ones(1))\n",
    "        self.g1 = nn.Parameter(torch.ones(1))\n",
    "        self.g2 = nn.Parameter(torch.ones(1))\n",
    "        self.g3 = nn.Parameter(torch.ones(1))\n",
    "        cnt = 0\n",
    "        if residual:\n",
    "            for g in self.gamma_reso:\n",
    "                if g != 0:\n",
    "                    self.gamma.append(g)\n",
    "                else:\n",
    "                    if cnt == 0:\n",
    "                        self.gamma.append(self.g0)\n",
    "                    if cnt == 1:\n",
    "                        self.gamma.append(self.g1)\n",
    "                    if cnt == 2:\n",
    "                        self.gamma.append(self.g2)\n",
    "                    if cnt == 3:\n",
    "                        self.gamma.append(self.g3)\n",
    "                    cnt += 1\n",
    "\n",
    "        self.num_patches = [int((in_size // p)** 2) for p in self.patch_size]\n",
    "        self.n_chunks = len(self.patch_size)\n",
    "        with torch.no_grad():\n",
    "            test_inp = torch.ones(4, in_channels, in_size, in_size)\n",
    "            test_chunks = torch.chunk(test_inp, chunks=self.n_chunks, dim=1)\n",
    "            self.chunk_channels = [chunk.shape[1] for chunk in test_chunks]\n",
    "        self.patch_dim = [int(self.chunk_channels[i] * (self.patch_size[i] ** 2)) for i in range(self.n_chunks)]\n",
    "\n",
    "        ############################# VIT #########################################\n",
    "        # Giảm chiều vector sau concat 2*patch_dim về D:\n",
    "        self.embedding = nn.ModuleList([])\n",
    "        for p_dim in self.patch_dim:\n",
    "            self.embedding.append(nn.Linear(p_dim, self.dim))\n",
    "        # transformer:\n",
    "        self.share_weight = share_weight\n",
    "        if not share_weight:\n",
    "            self.transformers = nn.ModuleList([])\n",
    "            for _ in range(len(self.patch_size)):\n",
    "                if useKNN == 0:\n",
    "                    print(\"use vanilla attention\")\n",
    "                    self.transformers.append(Transformer(self.dim, self.depth, self.heads, self.dim_head, self.mlp_dim, self.dropout_value))\n",
    "                elif useKNN > 0:\n",
    "                    print(\"use KNN attention: topK ratio: \", useKNN)\n",
    "                    self.transformers.append(kNNTransformer(self.dim, self.depth, self.heads, self.mlp_dim, self.dropout_value, useKNN))\n",
    "                else:\n",
    "                    print(\"error when use attention...\")\n",
    "        else:\n",
    "            if useKNN == 0:\n",
    "                print(\"use vanilla attention\")\n",
    "                self.transformer = Transformer(self.dim, self.depth, self.heads, self.dim_head, self.mlp_dim, self.dropout_value)\n",
    "            elif useKNN > 0:\n",
    "                print(\"use KNN attention: topK ratio: \", useKNN)\n",
    "                self.transformer = kNNTransformer(self.dim, self.depth, self.heads, self.mlp_dim, self.dropout_value, useKNN)\n",
    "            else:\n",
    "                print(\"error when use attention...\")\n",
    "        \n",
    "    def forward(self, features):\n",
    "        outputs = []\n",
    "        chunked_features = torch.chunk(features, self.n_chunks, dim=1)\n",
    "\n",
    "        for i in range(self.n_chunks):\n",
    "            # Flatten to vectors:\n",
    "            # print(\"shape: \", chunked_features[i].shape)\n",
    "            feature_vectors = self.flatten_to_vectors(feature=chunked_features[i], p_size=self.patch_size[i])      # B, num_patch, patch_dim\n",
    "            \n",
    "            # print(\"patchsize: \", self.patch_size[i])\n",
    "            # print(\"     Vectors shape: \", feature_vectors.shape)\n",
    "\n",
    "            # ViT:\n",
    "            embed = self.embedding[i](feature_vectors)                # B, num_patch, dim\n",
    "            if not self.share_weight:       \n",
    "                output = self.transformers[i](embed)                # B, num_patch, dim\n",
    "            else:\n",
    "                output = self.transformer(embed)\n",
    "            #\n",
    "            if self.residual:\n",
    "                output = embed + self.gamma[i] * output        # B, num_patch, dim\n",
    "            # print(\"     output shape: \", output.shape)\n",
    "            output = output.mean(dim = 1).squeeze(dim=1)          # B, 1, dim\n",
    "            outputs.append(output)\n",
    "        \n",
    "        out = torch.cat(outputs, dim=1)\n",
    "        # print(\"multi shape: \", out.shape)\n",
    "        return out\n",
    "\n",
    "    def flatten_to_vectors(self, feature=None, p_size=1):\n",
    "        return rearrange(feature, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p_size, p2=p_size)\n",
    "\n",
    "    \n",
    "    \n",
    "class PairwiseCNNCMultiViT(nn.Module):\n",
    "    def __init__(self, image_size=224, num_classes=1, \\\n",
    "                dim=1024, depth=6, heads=8, mlp_dim=2048, dim_head=64, dropout=0.15,\\\n",
    "                backbone='xception_net', pretrained=True,unfreeze_blocks=-1,\\\n",
    "                patch_reso='1-2-4-8', gammaagg_reso='0.8_0.4_0.2_0.1',\\\n",
    "                features_at_block='10',\\\n",
    "                dropout_in_mlp=0.0, residual=True, transformer_shareweight=True, useKNN=0, \\\n",
    "                embedding_return='mlp_hidden', freq_stream=True):  \n",
    "\n",
    "        super(PairwiseCNNCMultiViT, self).__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.num_classes = num_classes\n",
    "        self.backbone = backbone\n",
    "        self.embedding_return = embedding_return\n",
    "\n",
    "        self.features_size = {\n",
    "            'efficient_net': {\n",
    "                '0': (16, 64, 64),\n",
    "                '1': (24, 32, 32),\n",
    "                '2': (24, 32, 32),\n",
    "                '3': (40, 16, 16),\n",
    "                '4': (40, 16, 16),\n",
    "                '5': (80, 8, 8),\n",
    "                '6': (80, 8, 8),\n",
    "                '7': (80, 8, 8),\n",
    "                '8': (112, 8, 8),\n",
    "                '9': (112, 8, 8),\n",
    "                '10': (112, 8, 8),\n",
    "                '11': (192, 4, 4),\n",
    "                '12': (192, 4, 4),\n",
    "                '13': (192, 4, 4),\n",
    "                '14': (192, 4, 4),\n",
    "                '15': (320, 4, 4),\n",
    "                'final': (1280, 4, 4)\n",
    "            },\n",
    "            'xception_net': {\n",
    "                'final': (2048, 4, 4)\n",
    "            }\n",
    "        }\n",
    "        self.out_ext_channels = self.features_size[backbone][features_at_block][0]\n",
    "        self.out_ext_size = self.features_size[backbone][features_at_block][1]\n",
    "\n",
    "        self.pretrained = pretrained\n",
    "        self.features_at_block = features_at_block\n",
    "\n",
    "        self.extractor = self.get_feature_extractor(architecture=backbone, pretrained=pretrained, unfreeze_blocks=unfreeze_blocks, num_classes=num_classes, in_channels=(3 if not freq_stream else 1))   # efficient_net-b0, return shape (1280, 8, 8) or (1280, 7, 7)\n",
    "\n",
    "        self.multi_transformer = CMultiscaleViT(in_channels=self.out_ext_channels, in_size=self.out_ext_size, patch_reso=patch_reso, gamma_reso=gammaagg_reso,\\\n",
    "                                          useKNN=useKNN, depth=depth, heads=heads, dim=dim, mlp_dim=mlp_dim, dim_head=dim_head, dropout=dropout, residual=residual, share_weight=transformer_shareweight)\n",
    "\n",
    "        self.mlp_relu = nn.ReLU(inplace=True)\n",
    "        self.mlp_head_hidden = nn.Linear(len(patch_reso.split('-')) * dim, mlp_dim)\n",
    "        self.mlp_dropout = nn.Dropout(dropout_in_mlp)\n",
    "        self.mlp_head_out = nn.Linear(mlp_dim, self.num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def get_feature_extractor(self, architecture=\"efficient_net\", unfreeze_blocks=-1, pretrained=False, num_classes=1, in_channels=3):\n",
    "        extractor = None\n",
    "        if architecture == \"efficient_net\":\n",
    "            extractor = EfficientNet.from_pretrained('efficientnet-b0', num_classes=num_classes,in_channels = in_channels, pretrained=bool(pretrained))\n",
    "            if unfreeze_blocks != -1:\n",
    "                # Freeze the first (num_blocks - 3) blocks and unfreeze the rest \n",
    "                for i in range(0, len(extractor._blocks)):\n",
    "                    for index, param in enumerate(extractor._blocks[i].parameters()):\n",
    "                        if i >= len(extractor._blocks) - unfreeze_blocks:\n",
    "                            param.requires_grad = True\n",
    "                        else:\n",
    "                            param.requires_grad = False\n",
    "        \n",
    "        if architecture == 'xception_net':\n",
    "            xception = get_model(\"xception\", pretrained=bool(pretrained))\n",
    "            extractor = nn.Sequential(*list(xception.children())[:-1])\n",
    "            extractor[0].final_block.pool = nn.Identity()\n",
    "            if in_channels != 3:\n",
    "                extractor[0].init_block.conv1.conv = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
    "\n",
    "            if unfreeze_blocks != -1:\n",
    "                blocks = len(extractor[0].children())\n",
    "                print(\"Number of blocks in xception: \", len(blocks))\n",
    "                for i, block in enumerate(extractor[0].children()):\n",
    "                    if i >= blocks - unfreeze_blocks:\n",
    "                        for param in block.parameters():\n",
    "                            param.requires_grad = True\n",
    "                    else:\n",
    "                        for param in block.parameters():\n",
    "                            param.requires_grad = False\n",
    "        print(\"Pretrained backbone: \", bool(pretrained))\n",
    "        return extractor\n",
    "\n",
    "\n",
    "    def extract_feature(self, imgs):\n",
    "        if self.backbone == 'efficient_net':\n",
    "            if self.features_at_block == 'final':\n",
    "                features = self.extractor.extract_features(imgs)                 # shape (batchsize, 1280, 8, 8)\n",
    "            else:\n",
    "                features = self.extractor.extract_features_at_block(imgs, int(self.features_at_block))\n",
    "        else:\n",
    "            features = self.extractor(imgs)\n",
    "        return features\n",
    "\n",
    "    def forward_once(self, imgs):\n",
    "        features = self.extract_feature(imgs)\n",
    "        # print(\"Features shape: \", features.shape)\n",
    "\n",
    "        ##### Forward to ViT\n",
    "        e = None\n",
    "        e1 = self.multi_transformer(features)     # B, number_of_patch * D\n",
    "\n",
    "        x = self.mlp_dropout(e1)         # B, number_of_patch * D\n",
    "        e2 = self.mlp_head_hidden(x)     # B, number_of_patch * D => B, mlp_dim\n",
    "        x = self.mlp_relu(e2)\n",
    "        x = self.mlp_dropout(x)\n",
    "        e3 = self.mlp_head_out(x)        # B, mlp_dim => B, 1\n",
    "        x = self.sigmoid(e3)\n",
    "        if self.embedding_return == 'mlp_before':\n",
    "            e = e1\n",
    "        elif self.embedding_return == 'mlp_hidden':\n",
    "            e = e2\n",
    "        elif self.embedding_return == 'mlp_out':\n",
    "            e = e3\n",
    "        return e, x\n",
    "\n",
    "    def forward(self, imgs0, imgs1):\n",
    "        embedding_0, out_0 = self.forward_once(imgs0)\n",
    "        embedding_1, out_1 = self.forward_once(imgs1)\n",
    "        return embedding_0, out_0, embedding_1, out_1\n",
    "\n",
    "from torchsummary import summary\n",
    "if __name__ == '__main__':\n",
    "    x = torch.ones(32, 3, 128, 128)\n",
    "    model_ = PairwiseCNNCMultiViT(image_size=128, num_classes=1, \\\n",
    "                dim=512, depth=3, heads=4, mlp_dim=1024, dim_head=64, dropout=0.15,\\\n",
    "                backbone='efficient_net', pretrained=True,unfreeze_blocks=-1,\\\n",
    "                patch_reso='1-2-4', gammaagg_reso='0.2_0.2_0',\\\n",
    "                features_at_block='11',\\\n",
    "                dropout_in_mlp=0.0, residual=True, transformer_shareweight=False, useKNN=0, \\\n",
    "                embedding_return='mlp_hidden', freq_stream='rgb')\n",
    "\n",
    "    e0, out0, e1, out1 = model_(x, x)\n",
    "    print(e0.shape)\n",
    "    print(out0.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('phucnp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73e4523d5c5fcabc881bfbabdc03d28b885253c65d62f8f3eb31939c7679911f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "tensor([[-0.4331,  1.6764],\n",
      "        [-1.5141, -0.2475],\n",
      "        [-1.9706, -0.1885],\n",
      "        [ 0.5792,  0.3717],\n",
      "        [ 0.3214,  0.0276],\n",
      "        [ 0.4147,  0.5795],\n",
      "        [-0.8673,  0.7784],\n",
      "        [ 0.1788,  0.8558]], grad_fn=<AddmmBackward>)\n",
      "tensor([1, 0, 1, 1, 0, 1, 0, 1])\n",
      "tensor([1, 1, 1, 0, 0, 1, 1, 1])\n",
      "0.625\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from model.backbone.efficient_net.model import EfficientNet\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class EfficientSuppression(nn.Module):\n",
    "    def __init__(self, pretrained=False, features_at_block='8'):\n",
    "        super(EfficientSuppression, self).__init__()\n",
    "\n",
    "        self.features_size = {\n",
    "            '0': (16, 64, 64),\n",
    "            '1': (24, 32, 32),\n",
    "            '2': (24, 32, 32),\n",
    "            '3': (40, 16, 16),\n",
    "            '4': (40, 16, 16),\n",
    "            '5': (80, 8, 8),\n",
    "            '6': (80, 8, 8),\n",
    "            '7': (80, 8, 8),\n",
    "            '8': (112, 8, 8),\n",
    "            '9': (112, 8, 8),\n",
    "            '10': (112, 8, 8),\n",
    "            '11': (192, 4, 4),\n",
    "            '12': (192, 4, 4),\n",
    "            '13': (192, 4, 4),\n",
    "            '14': (192, 4, 4),\n",
    "            '15': (320, 4, 4),\n",
    "            'final': (1280, 4, 4)\n",
    "        }\n",
    "\n",
    "        self.efficient = EfficientNet.from_pretrained('efficientnet-b0', num_classes=2, in_channels = 3,pretrained=pretrained)\n",
    "        # print(self.efficient)\n",
    "        self.features_at_block = features_at_block\n",
    "        self.final = True if features_at_block == 'final' else False\n",
    "        if not self.final:\n",
    "            self._conv_head = self.efficient.get_conv(in_channel=self.features_size[features_at_block][0], out_channel=1280)\n",
    "            self._bn1 = self.efficient._bn1\n",
    "            self._avg_pooling = self.efficient._avg_pooling\n",
    "            self._dropout = self.efficient._dropout\n",
    "            self._fc = self.efficient._fc\n",
    "            self._swish = self.efficient._swish\n",
    "\n",
    "            for i in range(int(self.features_at_block) + 1, 16):\n",
    "                self.efficient._blocks[i] = Identity()\n",
    "\n",
    "        # print(self.efficient)\n",
    "\n",
    "    def forward(self, rgb):\n",
    "        if not self.final:\n",
    "            x = self.efficient.extract_features_at_block(rgb, selected_block=int(self.features_at_block))\n",
    "            x = self._conv_head(x)\n",
    "            x = self._bn1(x)\n",
    "            x = self._avg_pooling(x)\n",
    "            x = x.squeeze(dim=-1).squeeze(dim=-1)\n",
    "            x = self._dropout(x)\n",
    "            x = self._fc(x)\n",
    "        else:\n",
    "            x = self.efficient(rgb)\n",
    "        return x\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    torch.manual_seed(0)\n",
    "    for t in [str(i) for i in range(5, 6)]:\n",
    "        model = EfficientSuppression(pretrained=True, features_at_block=t)\n",
    "        x = torch.rand(8, 3, 128, 128)\n",
    "        label = torch.randint(low=0, high=2, size=(8,))\n",
    "        out = model(x)\n",
    "        values, preds = torch.max(out, dim=1)\n",
    "        print(out)\n",
    "        print(label.data)\n",
    "        print(preds)\n",
    "        accurate = torch.mean((label.data == preds), dtype=torch.float32).item()\n",
    "        print(accurate)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import einsum\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from einops import rearrange\n",
    "\n",
    "import sys\n",
    "from model.backbone.efficient_net.model import EfficientNet\n",
    "\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import re, math\n",
    "from model.vision_transformer.vit.vit import ViT, Transformer\n",
    "from model.vision_transformer.vit.kvit import kNNTransformer\n",
    "from model.vision_transformer.cnn_vit.efficient_vit import EfficientViT\n",
    "from pytorchcv.model_provider import get_model\n",
    "\n",
    "class CALayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16, topk_rate=0.5):\n",
    "        super(CALayer, self).__init__()\n",
    "        # global average pooling: feature --> point\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        # feature channel downscale and upscale --> channel weight\n",
    "        self.conv_du = nn.Sequential(\n",
    "            nn.Conv2d(channel, channel // reduction, 1, padding=0, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channel // reduction, channel, 1, padding=0, bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.channel = channel\n",
    "        self.topk_rate = topk_rate\n",
    "        self.topk = int(channel * topk_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x: B, C, W, H\n",
    "        # y: B, C, 1, 1\n",
    "        y = self.avg_pool(x)\n",
    "        attn_weight = self.conv_du(y)   # B, C, 1, 1\n",
    "        attn = attn_weight * x\n",
    "        attnw_idx = torch.topk(input=attn_weight,k=self.topk,dim=1,largest=True, sorted=False).indices  # B, k, 1, 1\n",
    "        attnw_idx = torch.sort(attnw_idx, dim=1).values\n",
    "        attnw_idx = attnw_idx.expand(-1, -1, x.shape[2], x.shape[3])     \n",
    "        attn = torch.gather(attn, dim=1, index=attnw_idx)\n",
    "        return attn\n",
    "\n",
    "\n",
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, relu=True,\n",
    "                 bn=True, bias=False):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.out_channels = out_planes\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                              dilation=dilation, groups=groups, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_planes, eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
    "        self.relu = nn.ReLU() if relu else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ChannelPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.cat((torch.max(x, 1)[0].unsqueeze(1), torch.mean(x, 1).unsqueeze(1)), dim=1)\n",
    "\n",
    "class spatial_attn_layer(nn.Module):\n",
    "    def __init__(self, kernel_size=3):\n",
    "        super(spatial_attn_layer, self).__init__()\n",
    "        self.compress = ChannelPool()\n",
    "        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size - 1) // 2, relu=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # import pdb;pdb.set_trace()\n",
    "        x_compress = self.compress(x)\n",
    "        x_out = self.spatial(x_compress)\n",
    "        scale = torch.sigmoid(x_out)  # broadcasting\n",
    "        return x * scale\n",
    "\n",
    "class DAB(nn.Module):\n",
    "    def __init__(self, n_feat: int, reduction: int, topk_rate=0.5, act_dab=None, dab_modules='sa-ca'):\n",
    "        super(DAB, self).__init__()\n",
    "        self.use_sa = True if 'sa' in dab_modules else False\n",
    "        self.use_ca = True if 'ca' in dab_modules else False\n",
    "        self.dab_modules = dab_modules\n",
    "        self.SA = spatial_attn_layer() if self.use_sa else None             ## Spatial Attention\n",
    "        self.CA = CALayer(n_feat, reduction, topk_rate)  if self.use_ca else None      ## Channel Attention\n",
    "        self.conv1x1_1 = nn.Conv2d(n_feat * 2, n_feat, kernel_size=1)\n",
    "        # self.conv1x1_2 = nn.Conv2d(n_feat, n_feat, kernel_size=1)\n",
    "        self.conv1x1_3 = nn.Conv2d(int(n_feat * (1 + topk_rate)), n_feat, kernel_size=1)\n",
    "        self.act = act_dab\n",
    "        self.topk_rate = topk_rate\n",
    "\n",
    "    def forward(self, ifreq, rgb):\n",
    "        if self.use_sa:\n",
    "            sa_branch = self.SA(ifreq)\n",
    "        if self.use_ca:\n",
    "            ca_branch = self.CA(ifreq)\n",
    "\n",
    "        if self.use_sa and self.use_ca:\n",
    "            attn = torch.cat([sa_branch, ca_branch], dim=1)\n",
    "        if self.use_sa and not self.use_ca:\n",
    "            attn = sa_branch\n",
    "        if not self.use_sa and self.use_ca:\n",
    "            attn = ca_branch\n",
    "\n",
    "        # print(attn.shape)\n",
    "        if '-' in self.dab_modules:\n",
    "            attn = self.conv1x1_1(attn)\n",
    "\n",
    "        # print(\"        attn shape: \", rgb.shape, attn.shape)\n",
    "        res = torch.cat([rgb, attn], dim=1)\n",
    "        res = self.conv1x1_3(res)\n",
    "        if self.act:\n",
    "            res = self.act(res)\n",
    "        return res\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, version='ca-fcat-0.5', in_dim=1024, activation=None, inner_dim=0, prj_out=False, qkv_embed=True):\n",
    "        super().__init__()\n",
    "        self.version = version\n",
    "        self.use_freq = True if self.version.split('-')[1][0] == 'f' else False\n",
    "        self.in_dim = in_dim\n",
    "        self.qkv_embed = qkv_embed\n",
    "        self.to_out = nn.Identity()\n",
    "        self.activation = activation\n",
    "        if self.qkv_embed:\n",
    "            inner_dim = self.in_dim if inner_dim == 0 else inner_dim\n",
    "            self.to_k = nn.Linear(in_dim, inner_dim, bias=False)\n",
    "            self.to_v = nn.Linear(in_dim, inner_dim, bias = False)\n",
    "            self.to_q = nn.Linear(in_dim, inner_dim, bias = False)\n",
    "            self.to_out = nn.Sequential(\n",
    "                nn.Linear(inner_dim, in_dim),\n",
    "                nn.Dropout(p=0.1)\n",
    "            ) if prj_out else nn.Identity()\n",
    "        print(\"freq combine: \", self.use_freq)\n",
    "\n",
    "    def forward(self, rgb, freq, ifreq):\n",
    "        \"\"\"\n",
    "            x ~ rgb_vectors: (b, n, in_dim)\n",
    "            y ~ freq_vectors: (b, n, in_dim)\n",
    "            z ~ freq_vectors: (b, n, in_dim)\n",
    "            Returns:\n",
    "                attn_weight: (b, n, n)\n",
    "                attn_output: (b, n, in_dim)\n",
    "        \"\"\"\n",
    "        if self.qkv_embed:\n",
    "            q = self.to_q(rgb)\n",
    "            k = self.to_k(freq)\n",
    "            v = self.to_v(ifreq)\n",
    "        else:\n",
    "            q, k, v = rgb, freq, ifreq\n",
    "        attn_rgb_to_ifreq, attnweight_rgb_to_ifreq = self.scale_dot(q, k, v, dropout_p=0.00)\n",
    "        if self.use_freq:\n",
    "            attn_rgb_to_freq = torch.bmm(attnweight_rgb_to_ifreq, freq)\n",
    "            attn_out = self.to_out(attn_rgb_to_freq)\n",
    "        else:\n",
    "            attn_out = self.to_out(attn_rgb_to_ifreq)\n",
    "            \n",
    "        fusion_out = self.fusion(rgb, attn_out)\n",
    "        if self.activation is not None:\n",
    "            fusion_out = self.activation(fusion_out)\n",
    "        return fusion_out\n",
    "\n",
    "    \"\"\"\n",
    "        Get from torch.nn.MultiheadAttention\n",
    "        scale-dot: https://github.com/pytorch/pytorch/blob/1c5a8125798392f8d7c57e88735f43a14ae0beca/torch/nn/functional.py#L4966\n",
    "        multi-head: https://github.com/pytorch/pytorch/blob/1c5a8125798392f8d7c57e88735f43a14ae0beca/torch/nn/functional.py#L5059\n",
    "    \"\"\"\n",
    "    def scale_dot(self, q, k, v, attn_mask=None, dropout_p=0):\n",
    "        B, Nt, E = q.shape\n",
    "        q = q / math.sqrt(E)\n",
    "        # (B, Nt, E) x (B, E, Ns) -> (B, Nt, Ns)\n",
    "        attn = torch.bmm(q, k.transpose(-2, -1))\n",
    "        if attn_mask is not None:\n",
    "            attn += attn_mask\n",
    "        attn = torch.nn.functional.softmax(attn, dim=-1)\n",
    "        if dropout_p > 0.0:\n",
    "            attn = torch.nn.functional.dropout(attn, p=dropout_p)\n",
    "        # (B, Nt, Ns) x (B, Ns, E) -> (B, Nt, E)\n",
    "        output = torch.bmm(attn, v)\n",
    "        return output, attn\n",
    "\n",
    "    def fusion(self, rgb, out_attn):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            rgb --      b, n, d\n",
    "            out_attn -- b, n, d\n",
    "        \"\"\"\n",
    "        weight = float(self.version.split('-')[-1])\n",
    "        if 'cat' in self.version:\n",
    "            out = torch.cat([rgb, weight * out_attn], dim=2)\n",
    "        elif 'add' in self.version:\n",
    "            out = torch.add(rgb, weight * out_attn)\n",
    "        return out\n",
    "\n",
    "class CMultiscaleViT(nn.Module):\n",
    "    def __init__(self, in_channels=112, in_size=8, patch_reso='1-2-4-8', gamma_reso='0.8_0.4_0.2_0.1', residual=True,\\\n",
    "                qkv_embed=True, prj_out=True, activation=None, fusca_version='ca-fcat-0.5', \\\n",
    "                useKNN=True, depth=6, heads=8, dim=1024, mlp_dim=2048, dim_head=64, dropout=0.15, share_weight=True):\n",
    "        super(CMultiscaleViT, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.heads = heads\n",
    "        self.dim_head = dim_head\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.dropout_value = dropout\n",
    "\n",
    "        self.fusca_version = fusca_version\n",
    "        self.residual = residual\n",
    "        self.patch_reso = patch_reso\n",
    "        self.gamma_reso = gamma_reso\n",
    "\n",
    "        self.patch_size = list(map(int, patch_reso.split('-')))\n",
    "        self.gamma_reso = list(map(float, gamma_reso.split('_')))\n",
    "        self.gamma = []\n",
    "        self.g0 = nn.Parameter(torch.ones(1))\n",
    "        self.g1 = nn.Parameter(torch.ones(1))\n",
    "        self.g2 = nn.Parameter(torch.ones(1))\n",
    "        self.g3 = nn.Parameter(torch.ones(1))\n",
    "        cnt = 0\n",
    "        if residual:\n",
    "            for g in self.gamma_reso:\n",
    "                if g != 0:\n",
    "                    self.gamma.append(g)\n",
    "                else:\n",
    "                    if cnt == 0:\n",
    "                        self.gamma.append(self.g0)\n",
    "                    if cnt == 1:\n",
    "                        self.gamma.append(self.g1)\n",
    "                    if cnt == 2:\n",
    "                        self.gamma.append(self.g2)\n",
    "                    if cnt == 3:\n",
    "                        self.gamma.append(self.g3)\n",
    "                    cnt += 1\n",
    "\n",
    "        self.num_patches = [int((in_size // p)** 2) for p in self.patch_size]\n",
    "        self.n_chunks = len(self.patch_size)\n",
    "        with torch.no_grad():\n",
    "            test_inp = torch.ones(4, in_channels, in_size, in_size)\n",
    "            test_chunks = torch.chunk(test_inp, chunks=self.n_chunks, dim=1)\n",
    "            self.chunk_channels = [chunk.shape[1] for chunk in test_chunks]\n",
    "        self.patch_dim = [int(self.chunk_channels[i] * (self.patch_size[i] ** 2)) for i in range(self.n_chunks)]\n",
    "\n",
    "\n",
    "        ############################# CROSS ATTENTION #############################\n",
    "        self.cross_attention = nn.ModuleList([])\n",
    "        for p_dim in self.patch_dim:\n",
    "            self.cross_attention.append(CrossAttention(version=fusca_version, in_dim=p_dim, activation=activation, inner_dim=0, prj_out=prj_out, qkv_embed=qkv_embed))\n",
    "\n",
    "        ############################# VIT #########################################\n",
    "        # Giảm chiều vector sau concat 2*patch_dim về D:\n",
    "        self.embedding = nn.ModuleList([])\n",
    "        for p_dim in self.patch_dim:\n",
    "            if 'cat' in self.fusca_version:\n",
    "                self.embedding.append(nn.Linear(2 * p_dim, self.dim))\n",
    "            else:\n",
    "                self.embedding.append(nn.Linear(p_dim, self.dim))\n",
    "        # transformer:\n",
    "        self.share_weight = share_weight\n",
    "        if not share_weight:\n",
    "            self.transformers = nn.ModuleList([])\n",
    "            for _ in range(len(self.patch_size)):\n",
    "                if useKNN == 0:\n",
    "                    print(\"use vanilla attention\")\n",
    "                    self.transformers.append(Transformer(self.dim, self.depth, self.heads, self.dim_head, self.mlp_dim, self.dropout_value))\n",
    "                elif useKNN > 0:\n",
    "                    print(\"use KNN attention: topK ratio: \", useKNN)\n",
    "                    self.transformers.append(kNNTransformer(self.dim, self.depth, self.heads, self.mlp_dim, self.dropout_value, useKNN))\n",
    "                else:\n",
    "                    print(\"error when use attention...\")\n",
    "        else:\n",
    "            if useKNN == 0:\n",
    "                print(\"use vanilla attention\")\n",
    "                self.transformer = Transformer(self.dim, self.depth, self.heads, self.dim_head, self.mlp_dim, self.dropout_value)\n",
    "            elif useKNN > 0:\n",
    "                print(\"use KNN attention: topK ratio: \", useKNN)\n",
    "                self.transformer = kNNTransformer(self.dim, self.depth, self.heads, self.mlp_dim, self.dropout_value, useKNN)\n",
    "            else:\n",
    "                print(\"error when use attention...\")\n",
    "        \n",
    "    def forward(self, rgb_features, freq_features, ifreq_features):\n",
    "        outputs = []\n",
    "        rgbs = torch.chunk(rgb_features, self.n_chunks, dim=1)\n",
    "        freqs = torch.chunk(freq_features, self.n_chunks, dim=1)\n",
    "        ifreqs = torch.chunk(ifreq_features, self.n_chunks, dim=1)\n",
    "\n",
    "        for i in range(self.n_chunks):\n",
    "            # Flatten to vectors:\n",
    "            # print(\"shape: \", rgbs[i].shape, \" \", freqs[i].shape, \" \", ifreqs[i].shape)\n",
    "            rgb_vectors = self.flatten_to_vectors(feature=rgbs[i], p_size=self.patch_size[i])      # B, num_patch, patch_dim\n",
    "            freq_vectors = self.flatten_to_vectors(feature=freqs[i], p_size=self.patch_size[i])    # B, num_patch, patch_dim\n",
    "            ifreq_vectors = self.flatten_to_vectors(feature=ifreqs[i], p_size=self.patch_size[i])  # B, num_patch, patch_dim\n",
    "            # print(\"patchsize: \", self.patch_size[i])\n",
    "            # print(\"     Vectors shape: \", rgb_vectors.shape, freq_vectors.shape, ifreq_vectors.shape)\n",
    "\n",
    "            # Cross attention:\n",
    "            attn_out = self.cross_attention[i](rgb_vectors, freq_vectors, ifreq_vectors)    # B, num_patch, patch_dim/2*patch_dim\n",
    "            # print(\"     attn out shape: \", attn_out.shape)\n",
    "\n",
    "            # ViT:\n",
    "            embed = self.embedding[i](attn_out)                # B, num_patch, dim\n",
    "            if not self.share_weight:       \n",
    "                output = self.transformers[i](embed)                # B, num_patch, dim\n",
    "            else:\n",
    "                output = self.transformer(embed)\n",
    "            if self.residual:\n",
    "                output = embed + self.gamma[i] * output        # B, num_patch, dim\n",
    "            # print(\"     output shape: \", output.shape)\n",
    "            output = output.mean(dim = 1).squeeze(dim=1)          # B, 1, dim\n",
    "            outputs.append(output)\n",
    "        \n",
    "        out = torch.cat(outputs, dim=1)\n",
    "        # print(\"multi shape: \", out.shape)\n",
    "        return out\n",
    "\n",
    "    def flatten_to_vectors(self, feature=None, p_size=1):\n",
    "        return rearrange(feature, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p_size, p2=p_size)\n",
    "\n",
    "    \n",
    "class PairwiseDualDabCNNCMultiViT(nn.Module):\n",
    "    def __init__(self, image_size=224, num_classes=1, \\\n",
    "                dim=1024, depth=6, heads=8, mlp_dim=2048, dim_head=64, dropout=0.15,\\\n",
    "                backbone='xception_net', pretrained=True,unfreeze_blocks=-1,\\\n",
    "                normalize_ifft='batchnorm',\\\n",
    "                qkv_embed=True, prj_out=False, act='none',\\\n",
    "                patch_reso='1-2-4-8', gammaagg_reso='0.8_0.4_0.2_0.1',\\\n",
    "                fusca_version='ca-fcat-0.5',\\\n",
    "                features_at_block='10', \\\n",
    "                dropout_in_mlp=0.0, residual=True, transformer_shareweight=True, \\\n",
    "                act_dab='none', topk_channels=0.5, dab_modules='sa-ca', dabifft_normalize='none', dab_blocks='1_3_6_9', \\\n",
    "                embedding_return='mlp_hidden', useKNN=0):  \n",
    "\n",
    "        super(PairwiseDualDabCNNCMultiViT, self).__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.num_classes = num_classes\n",
    "        self.backbone = backbone\n",
    "        if 'efficient_net' in backbone:\n",
    "            dab_blocks = '-1_' + dab_blocks\n",
    "        self.dab_blocks = sorted(list(map(int, dab_blocks.split('_'))))\n",
    "        print(\"dab blocks: \", self.dab_blocks)\n",
    "        self.dabifft_normalize = dabifft_normalize\n",
    "        self.last_block = int(features_at_block) if features_at_block != 'final' else 15\n",
    "\n",
    "        self.features_size = {\n",
    "            'efficient_net': {\n",
    "                '0': (16, 64, 64),\n",
    "                '1': (24, 32, 32),\n",
    "                '2': (24, 32, 32),\n",
    "                '3': (40, 16, 16),\n",
    "                '4': (40, 16, 16),\n",
    "                '5': (80, 8, 8),\n",
    "                '6': (80, 8, 8),\n",
    "                '7': (80, 8, 8),\n",
    "                '8': (112, 8, 8),\n",
    "                '9': (112, 8, 8),\n",
    "                '10': (112, 8, 8),\n",
    "                '11': (192, 4, 4),\n",
    "                '12': (192, 4, 4),\n",
    "                '13': (192, 4, 4),\n",
    "                '14': (192, 4, 4),\n",
    "                '15': (320, 4, 4),\n",
    "                'final': (1280, 4, 4)\n",
    "            },\n",
    "            'xception_net': {\n",
    "                'final': (2048, 4, 4)\n",
    "            }\n",
    "        }\n",
    "        self.out_ext_channels = self.features_size[backbone][features_at_block][0]\n",
    "        self.out_ext_size = self.features_size[backbone][features_at_block][1]\n",
    "        self.fusca_version = fusca_version  # in ['ca-rgb_cat-0.5', 'ca-freq_cat-0.5']\n",
    "        self.activation = self.get_activation(act)\n",
    "        self.dab_activation = self.get_activation(act_dab)\n",
    "\n",
    "        self.pretrained = pretrained\n",
    "        self.features_at_block = features_at_block\n",
    "        self.rgb_extractor = self.get_feature_extractor(architecture=backbone, pretrained=pretrained, unfreeze_blocks=unfreeze_blocks, num_classes=num_classes, in_channels=3)   # efficient_net-b0, return shape (1280, 8, 8) or (1280, 7, 7)\n",
    "        self.freq_extractor = self.get_feature_extractor(architecture=backbone, pretrained=pretrained, unfreeze_blocks=unfreeze_blocks, num_classes=num_classes, in_channels=1)     \n",
    "        self.normalize_ifft = normalize_ifft\n",
    "        if self.normalize_ifft == 'batchnorm':\n",
    "            self.batchnorm_ifft = nn.BatchNorm2d(num_features=self.out_ext_channels)\n",
    "        if self.normalize_ifft == 'layernorm':\n",
    "            self.layernorm_ifft = nn.LayerNorm(normalized_shape=self.features_size[backbone][features_at_block])\n",
    "        \n",
    "        # DAB Block:\n",
    "        num_dab = len(self.dab_blocks) - 1\n",
    "        self.dab = nn.ModuleList([])\n",
    "        for i in range(num_dab):\n",
    "            at_block = self.dab_blocks[i+1]\n",
    "            in_features = self.features_size[backbone][str(at_block)][0]\n",
    "            self.dab.append(DAB(n_feat=in_features, reduction=1, topk_rate=topk_channels, act_dab=self.dab_activation, dab_modules=dab_modules))\n",
    "\n",
    "        # Multi ViT:\n",
    "        self.multi_transformer = CMultiscaleViT(in_channels=self.out_ext_channels, in_size=self.out_ext_size, patch_reso=patch_reso, gamma_reso=gammaagg_reso,\\\n",
    "                                          qkv_embed=qkv_embed, prj_out=prj_out, activation=self.activation, fusca_version=fusca_version,\\\n",
    "                                          useKNN=useKNN, depth=depth, heads=heads, dim=dim, mlp_dim=mlp_dim, dim_head=dim_head, dropout=dropout, residual=residual, share_weight=transformer_shareweight)\n",
    "\n",
    "        self.mlp_relu = nn.ReLU(inplace=True)\n",
    "        self.mlp_head_hidden = nn.Linear(len(patch_reso.split('-')) * dim, mlp_dim)\n",
    "        self.mlp_dropout = nn.Dropout(dropout_in_mlp)\n",
    "        self.mlp_head_out = nn.Linear(mlp_dim, self.num_classes)\n",
    "        #\n",
    "        self.embedding_return = embedding_return\n",
    "\n",
    "    def get_activation(self, act):\n",
    "        if act == 'relu':\n",
    "            activation = nn.ReLU(inplace=True)\n",
    "        elif act == 'tanh':\n",
    "            activation = nn.Tanh()\n",
    "        elif act == 'sigmoid':\n",
    "            activation = nn.Sigmoid()\n",
    "        elif act == 'leakyrely':\n",
    "            activation = nn.LeakyReLU()\n",
    "        elif act == 'selu':\n",
    "            activation = nn.SELU()\n",
    "        elif act == 'gelu':\n",
    "            activation = nn.GELU()\n",
    "        else:\n",
    "            activation = None\n",
    "        return activation\n",
    "\n",
    "    def get_feature_extractor(self, architecture=\"efficient_net\", unfreeze_blocks=-1, pretrained=False, num_classes=1, in_channels=3):\n",
    "        extractor = None\n",
    "        if architecture == \"efficient_net\":\n",
    "            extractor = EfficientNet.from_pretrained('efficientnet-b0', num_classes=num_classes,in_channels = in_channels, pretrained=bool(pretrained))\n",
    "            if unfreeze_blocks != -1:\n",
    "                # Freeze the first (num_blocks - 3) blocks and unfreeze the rest \n",
    "                for i in range(0, len(extractor._blocks)):\n",
    "                    for index, param in enumerate(extractor._blocks[i].parameters()):\n",
    "                        if i >= len(extractor._blocks) - unfreeze_blocks:\n",
    "                            param.requires_grad = True\n",
    "                        else:\n",
    "                            param.requires_grad = False\n",
    "        \n",
    "        if architecture == 'xception_net':\n",
    "            xception = get_model(\"xception\", pretrained=bool(pretrained))\n",
    "            extractor = nn.Sequential(*list(xception.children())[:-1])\n",
    "            extractor[0].final_block.pool = nn.Identity()\n",
    "            if in_channels != 3:\n",
    "                extractor[0].init_block.conv1.conv = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
    "\n",
    "            if unfreeze_blocks != -1:\n",
    "                blocks = len(extractor[0].children())\n",
    "                print(\"Number of blocks in xception: \", len(blocks))\n",
    "                for i, block in enumerate(extractor[0].children()):\n",
    "                    if i >= blocks - unfreeze_blocks:\n",
    "                        for param in block.parameters():\n",
    "                            param.requires_grad = True\n",
    "                    else:\n",
    "                        for param in block.parameters():\n",
    "                            param.requires_grad = False\n",
    "        print(\"Pretrained backbone: \", bool(pretrained))\n",
    "        return extractor\n",
    "\n",
    "    def ifft(self, freq_feature, norm_type='none'):\n",
    "        ifreq_feature = torch.log(torch.abs(torch.fft.ifft2(torch.fft.ifftshift(freq_feature))) + 1e-10)  # Hơi ảo???\n",
    "        if norm_type == 'none':\n",
    "            pass\n",
    "        elif norm_type == 'batchnorm':\n",
    "            ifreq_feature = self.batchnorm_ifft(ifreq_feature)\n",
    "        elif norm_type == 'layernorm':\n",
    "            ifreq_feature = self.layernorm_ifft(ifreq_feature)\n",
    "        elif norm_type == 'normal':\n",
    "            ifreq_feature = F.normalize(ifreq_feature)\n",
    "        elif norm_type == 'no_ifft':\n",
    "            return freq_feature\n",
    "        return ifreq_feature\n",
    "\n",
    "\n",
    "    def extract_feature(self, rgb_imgs, freq_imgs):\n",
    "        if self.backbone == 'efficient_net':\n",
    "            # Conv stem:\n",
    "            rgb_features = self.rgb_extractor.extract_features_convstem(rgb_imgs)                 \n",
    "            freq_features = self.freq_extractor.extract_features_convstem(freq_imgs)\n",
    "            # print(\"Features shape: \", rgb_features.shape)\n",
    "            # DAB Block:\n",
    "            for i in range(len(self.dab_blocks) - 1):   # -1_1_3_6_9\n",
    "                                                        #  0 1 2 3 4\n",
    "                # print(\"dab_blocks: \", self.dab_blocks[i]+1, ' -> ', self.dab_blocks[i+1])\n",
    "                rgb_features = self.rgb_extractor.extract_features_block_inrange(rgb_features, from_block=self.dab_blocks[i]+1, to_block=self.dab_blocks[i+1])\n",
    "                freq_features = self.freq_extractor.extract_features_block_inrange(freq_features, from_block=self.dab_blocks[i]+1, to_block=self.dab_blocks[i+1])\n",
    "                # Attention, concat and reduce channels:\n",
    "                ifreq_features = self.ifft(freq_features, norm_type=self.dabifft_normalize)\n",
    "                # print(\" Shape: \", rgb_features.shape, ifreq_features.shape)\n",
    "                rgb_features = self.dab[i](ifreq_features, rgb_features)\n",
    "\n",
    "            # Last block:\n",
    "            rgb_features = self.rgb_extractor.extract_features_block_inrange(rgb_features, from_block=self.dab_blocks[-1]+1, to_block=self.last_block)\n",
    "            freq_features = self.freq_extractor.extract_features_block_inrange(freq_features, from_block=self.dab_blocks[-1]+1, to_block=self.last_block)\n",
    "            \n",
    "            # Convhead:\n",
    "            # print(\"After last block Features shape: \", rgb_features.shape, freq_features.shape)\n",
    "            if self.features_at_block == 'final':\n",
    "                rgb_features = self.rgb_extractor.extract_features_convhead(rgb_features)\n",
    "                freq_features = self.freq_extractor.extract_features_convhead(freq_features)\n",
    "        else:\n",
    "            rgb_features = self.rgb_extractor(rgb_imgs)\n",
    "            freq_features = self.freq_extractor(freq_imgs)\n",
    "        return rgb_features, freq_features\n",
    "\n",
    "    def forward_once(self, rgb_imgs, freq_imgs):\n",
    "        rgb_features, freq_features = self.extract_feature(rgb_imgs, freq_imgs)\n",
    "        ifreq_features = self.ifft(freq_features, norm_type=self.normalize_ifft)\n",
    "        # print(\"Features shape: \", rgb_features.shape, freq_features.shape, ifreq_features.shape)\n",
    "\n",
    "        ##### Forward to ViT\n",
    "        e1 = self.multi_transformer(rgb_features, freq_features, ifreq_features)     # B, number_of_patch * D\n",
    "\n",
    "        x = self.mlp_dropout(e1)         # B, number_of_patch * D\n",
    "        e2 = self.mlp_head_hidden(x)     # B, number_of_patch * D => B, mlp_dim\n",
    "        x = self.mlp_relu(e2)\n",
    "        x = self.mlp_dropout(x)\n",
    "        e3 = self.mlp_head_out(x)        # B, mlp_dim => B, 1\n",
    "        e = None\n",
    "        if self.embedding_return=='mlp_before':\n",
    "            e = e1\n",
    "        if self.embedding_return=='mlp_hidden':\n",
    "            e = e2\n",
    "        if self.embedding_return=='mlp_out':\n",
    "            e = e3\n",
    "        return e, e3\n",
    "\n",
    "    def forward(self, rgb_imgs0, freq_imgs0, rgb_imgs1, freq_imgs1):\n",
    "        embedding_0, out_0 = self.forward_once(rgb_imgs0, freq_imgs0)\n",
    "        embedding_1, out_1 = self.forward_once(rgb_imgs1, freq_imgs1)\n",
    "        # print(\"embed: \", embedding_0.shape, \"   out: \", out_0.shape)\n",
    "        return embedding_0, out_0, embedding_1, out_1\n",
    "\n",
    "from torchsummary import summary\n",
    "if __name__ == '__main__':\n",
    "    x = torch.ones(2, 3, 128, 128)\n",
    "    y = torch.ones(2, 1, 128, 128)\n",
    "    model_ = PairwiseDualDabCNNCMultiViT(image_size=128, num_classes=1, \\\n",
    "                dim=1024, depth=6, heads=8, mlp_dim=2048, dim_head=64, dropout=0.15,\\\n",
    "                backbone='efficient_net', pretrained=True,unfreeze_blocks=-1,\\\n",
    "                normalize_ifft='batchnorm',\\\n",
    "                qkv_embed=True, prj_out=False, act='selu',\\\n",
    "                patch_reso='1-2-4', gammaagg_reso='0.8_0.4_0.2',\\\n",
    "                fusca_version='ca-fcat-0.5',\\\n",
    "                features_at_block='11', \\\n",
    "                dropout_in_mlp=0.0, residual=True, transformer_shareweight=True, \\\n",
    "                act_dab='selu', topk_channels=0.5, dab_modules='ca', dabifft_normalize='normal', dab_blocks='0_3_5_6_10', \\\n",
    "                embedding_return=\"mlp_out\")\n",
    "    \n",
    "    import os\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "    device = torch.device('cuda' if not torch.cuda.is_available() else 'cpu')\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    model_ = model_.to(device)\n",
    "\n",
    "    # extractor = model_.rgb_extractor\n",
    "    # print(len(extractor._blocks))\n",
    "    # for idx in range(17):\n",
    "    #     print(\"after block \", idx, \" shape: \", extractor.extract_features_at_block(x, selected_block=idx).shape)\n",
    "    ### CHECK @@@\n",
    "    # print(\"\\nCheck @@@\")\n",
    "    # for idx in range(16):\n",
    "    #     print(\"after block \", idx, \" shape: \", extractor.extract_features_block_inrange(x, from_block=idx, to_block=idx+1).shape)\n",
    "    embed, out, _, _ = model_(x, y, x, y)\n",
    "    print(embed.shape)\n",
    "    print(out.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc if use min bce loss:  0.938118\n",
      "Test acc if use min total loss:  0.938118\n",
      "Best test acc:  0.959172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.938118, 0.938118)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os.path as osp\n",
    "\n",
    "def get_test_metric(step_ckcpoint_dir: str, model_type='pairwise', contain_fold=True):\n",
    "    test_csv = osp.join(step_ckcpoint_dir, 'result_test.csv')\n",
    "    val_csv = osp.join(step_ckcpoint_dir, 'result_val.csv')\n",
    "    # read:\n",
    "    test_df = pd.read_csv(test_csv)\n",
    "    val_df = pd.read_csv(val_csv)\n",
    "    #\n",
    "    best_test_acc = test_df[\" Test accuracy\"].max(skipna=True)\n",
    "    fold_info = step_ckcpoint_dir.split('/')[-2]\n",
    "    if '(' in fold_info and ')' in fold_info:\n",
    "        fold_info = fold_info.replace('(', '').replace(')', '').strip()\n",
    "        bestacc = float(fold_info.split('_')[-1])\n",
    "        if bestacc == \"{:.4f}\".format(best_test_acc):\n",
    "            print(\"Error some where!\")\n",
    "            print(\"best acc from fold: \", bestacc)\n",
    "            print(\"best acc from file: \", best_test_acc)\n",
    "            return None\n",
    "\n",
    "    if model_type == 'pairwise':\n",
    "        idx_min_bceloss, idx_min_totalloss = val_df[\" Val bce loss\"].idxmin(skipna=True), val_df[\" Val total loss\"].idxmin(skipna=True)\n",
    "        minbce_testdf = test_df.iloc[idx_min_bceloss]\n",
    "        mintotal_testdf = test_df.iloc[idx_min_totalloss]\n",
    "        print(\"Test acc if use min bce loss: \", minbce_testdf[\" Test accuracy\"])\n",
    "        print(\"Test acc if use min total loss: \", mintotal_testdf[\" Test accuracy\"])\n",
    "        print(\"Best test acc: \", test_df[\" Test accuracy\"].max(skipna=True))\n",
    "        return minbce_testdf[\" Test accuracy\"], mintotal_testdf[\" Test accuracy\"]\n",
    "    if model_type == 'normal':\n",
    "        idx_min_totalloss = val_df[\" Val loss\"].idxmin(skipna=True)\n",
    "        mintotal_testdf = test_df.iloc[idx_min_totalloss]\n",
    "        print(\"Test acc if use min total loss: \", mintotal_testdf[\" Test accuracy\"])\n",
    "        print(\"Best test acc: \", test_df[\" Test accuracy\"].max(skipna=True))\n",
    "        return mintotal_testdf[\" Test accuracy\"]\n",
    "\n",
    "\n",
    "step_ckcpoint = \"/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/checkpoint/datasetv5/Celeb-DFv6/kfold_pairwise_dual_dab_cnn_multivit/lr0.0002-0_b32_c0_esnone_lbce_nf5trick1_retmlp_hidden_im1.0mar0.2_vca-ifadd-0.8_KNN0_d1024md2048h4d3_bbefficient_netpre1_fatb11_pres1-2_res1_gres0.3_0.3_sh0_nrmbatchnorm_qkv1_prj1_actselunone_topk0.1_dabmca_dabinone_dabb8_9_10_sd0_dr0.3aug0/(0.1098_0.9705_0.1651_0.9592)_fold_1/step\"\n",
    "model_type = \"pairwise\"\n",
    "get_test_metric(step_ckcpoint, model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('phucnp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73e4523d5c5fcabc881bfbabdc03d28b885253c65d62f8f3eb31939c7679911f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
